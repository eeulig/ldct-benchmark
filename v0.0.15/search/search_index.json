{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#benchmarking-deep-learning-based-low-dose-ct-image-denoising-algorithms","title":"Benchmarking Deep Learning-Based Low Dose CT Image Denoising Algorithms","text":""},{"location":"#why-low-dose-ct","title":"Why low dose CT?","text":"<p>Computed tomography (CT) is an important imaging modality, with numerous applications including biology, medicine, and nondestructive testing. However, the use of ionizing radiation remains a key concern and thus clinical CT scans must follow the ALARA (as low as reasonably achievable) principle. Therefore, reducing the dose and thus radiation risk is of utmost importance and one of the primary research areas in the field. A straightforward way to reduce dose is by lowering the tube current (i.e., reducing the X-Ray intensity). However, this comes at the cost of deteriorated image quality due to increased image noise and thus potentially reduced diagnostic value. To alleviate this drawback, numerous algorithms have been proposed to solve the task of low-dose CT (LDCT) denoising, i.e., reducing image noise in the reconstructed image.</p>"},{"location":"#why-this-project","title":"Why this project?","text":"<p>We found that LDCT denoising algorithms proposed in the literature are difficult to compare due to the following reasons:</p> <p>Different datasetes</p> <p>Most LDCT denoising methods are trained and evaluated on (subsets of) one of the following two datasets</p> <ul> <li>2016 NIHAAPMMayo Clinic LDCT Grand Challenge<sup>1</sup></li> <li>LDCT and Projection data<sup>2</sup></li> </ul> <p>However, authors of each method employ their own training, validation, and test split and thus reported metrics are not comparable across publications.</p> <p>Unfair choice of hyperparameters</p> <p>Very few publications in the field report the use of hyperparameter optimization. And even if some form of hyperparameter optimization is employed, this is usually limited to the newly proposed method. For the comparison methods, authors often use the hyperparameters reported in the reference. However, the optimal hyperparameters for a given method may be dataset-specific, meaning that the parameters tuned by authors \\(A\\) for their dataset \\(\\mathcal{D}_A\\) might not generalize to another dataset \\(\\mathcal{D}_B\\) used by authors \\(B\\) in their experiments.</p> <p>Missing open source implementations</p> <p>Many authors don't provide open-source implementations of their algorithms and thus researchers are often left to implement comparison methods themselves. This increases the chance of errors and generally hinders reproducibility.</p> <p>Inadequate metrics</p> <p>Most LDCT denoising methods are evaluated using SSIM, peak signal-to-noise ratio (PSNR), or root-mean-square error (RMSE). While these are common metrics to quantify performance for natural image denoising, they are usually not in agreement with human readers for medical images.</p> <p>Therefore, the aim of this project is to </p> <ol> <li>provide a unified benchmark suite which serves as a reference for existing LDCT denoising algorithms (including optimal hyperparameters) and allows for a fair and reproducible comparison of new methods.</li> <li>make these algorithms easily accessible for practitioners by providing the trained models together with utility functions to denoise CT images.</li> <li>establish novel metrics for the evaluation of LDCT denoising algorithms.</li> </ol>"},{"location":"#contribute-an-algorithm","title":"Contribute an algorithm","text":"<p>We welcome contributions of novel denoising algorithms. For details on how to do so, please check out our contributing guide or reach out to me.</p>"},{"location":"#reference","title":"Reference","text":"<p>If you find this project useful for your work, please cite our arXiv preprint:</p> <p>Elias Eulig, Bj\u00f6rn Ommer, &amp; Marc Kachelrie\u00df (2024). Benchmarking Deep Learning-Based Low-Dose CT Image Denoising Algorithms. arXiv, 2401.04661.</p> <pre><code>@article{ldctbench-arxiv,\n  title = {Benchmarking Deep Learning-Based Low-Dose CT Image Denoising Algorithms}, \n  author = {Elias Eulig and Bj\u00f6rn Ommer and Marc Kachelrie\u00df},\n  year = {2024},\n  eprint = {2401.04661},\n  archivePrefix = {arXiv},\n  primaryClass = {physics.med-ph}\n}\n</code></pre> <p>or Medical Physics paper:</p> <p>Elias Eulig, Bj\u00f6rn Ommer, &amp; Marc Kachelrie\u00df (2024). Benchmarking Deep Learning-Based Low-Dose CT Image Denoising Algorithms. Medical Physics.</p> <pre><code>@article{ldctbench-medphys,\n  title = {Benchmarking deep learning-based low-dose CT image denoising algorithms},\n  author = {Eulig, Elias and Ommer, Bj\u00f6rn and Kachelrie\u00df, Marc},\n  journal = {Medical Physics},\n  year = {2024},\n  doi = {https://doi.org/10.1002/mp.17379},\n  url = {https://aapm.onlinelibrary.wiley.com/doi/abs/10.1002/mp.17379},\n  eprint = {https://aapm.onlinelibrary.wiley.com/doi/pdf/10.1002/mp.17379},\n}\n</code></pre> <ol> <li> <p>C. H. McCollough, A. C. Bartley, R. E. Carter, B. Chen, T. A. Drees, P. Edwards, D. R. Holmes III, A. E. Huang, F. Khan, S. Leng, K. L. McMillan, G. J. Michalak, K. M. Nunez, L. Yu, and J. G. Fletcher, \u201cLow-dose CT for the detection and classification of metastatic liver lesions: Results of the 2016 Low Dose CT Grand Challenge\u201d, Medical Physics, vol. 44, no. 10, pp. e339\u2013e352, 2017\u00a0\u21a9</p> </li> <li> <p>McCollough, C., Chen, B., Holmes III, D. R., Duan, X., Yu, Z., Yu, L., Leng, S., &amp; Fletcher, J. (2020). Low Dose CT Image and Projection Data (LDCT-and-Projection-data) (Version 6) [Data set]. The Cancer Imaging Archive. https://doi.org/10.7937/9NPB-2637.\u00a0\u21a9</p> </li> </ol>"},{"location":"denoising_algorithms/","title":"Denoising Algorithms","text":""},{"location":"denoising_algorithms/#implemented-algorithms","title":"Implemented algorithms","text":"<p>Below we list all methods currently implemented in our benchmark suite</p> Name Method name Paper CNN-10 cnn10 H. Chen, Y. Zhang, W. Zhang, P. Liao, K. Li, J. Zhou, and G. Wang, \"Low-dose CT via convolutional neural network,\u201d Biomedical Optics Express, vol. 8, no. 2, pp. 679\u2013694, Jan. 2017 RED-CNN redcnn H. Chen, Y. Zhang, M. K. Kalra, F. Lin, Y. Chen, P. Liao, J. Zhou, and G. Wang, \u201cLow-dose CT with a residual encoder-decoder convolutional neural network,\u201d IEEE Transactions on Medical Imaging, vol. 36, no. 12, pp. 2524\u20132535, Dec. 2017 WGAN-VGG wganvgg Q. Yang, P. Yan, Y. Zhang, H. Yu, Y. Shi, X. Mou, M. K. Kalra, Y. Zhang, L. Sun, and G. Wang, \u201cLow-dose CT image denoising using a generative adversarial network with wasserstein distance and perceptual loss,\u201d IEEE Transactions on Medical Imaging, vol. 37, no. 6, pp. 1348\u2013 1357, Jun. 2018. ResNet resnet A. D. Missert, S. Leng, L. Yu, and C. H. McCollough, \u201cNoise subtraction for low-dose CT images using a deep convolutional neural network,\u201d in Proceedings of the Fifth International Conference on Image Formation in X-Ray Computed Tomography, Salt Lake City, UT, USA, May 2018, pp. 399\u2013402. QAE qae F. Fan, H. Shan, M. K. Kalra, R. Singh, G. Qian, M. Getzin, Y. Teng, J. Hahn, and G. Wang, \u201cQuadratic autoencoder (Q-AE) for low-dose CT denoising,\u201d IEEE Transactions on Medical Imaging, vol. 39, no. 6, pp. 2035\u20132050, Jun. 2020. DU-GAN dugan Z. Huang, J. Zhang, Y. Zhang, and H. Shan, \u201cDU-GAN: Generative adversarial networks with dual-domain U-Net-based discriminators for low-dose CT denoising,\u201d IEEE Transactions on Instrumentation and Measurement, vol. 71, pp. 1\u201312, 2022. TransCT transct Z. Zhang, L. Yu, X. Liang, W. Zhao, and L. Xing, \u201cTransCT: Dual-path transformer for low dose computed tomography,\u201d in MICCAI, 2021 Trainable bilateral filter bilateral F. Wagner, M. Thies, M. Gu, Y. Huang, S. Pechmann, M. Patwari, S. Ploner, O. Aust, S. Uderhardt, G. Schett, S. Christiansen, and A. Maier, \u201cUltralow-parameter denoising: Trainable bilateral filter layers in computed tomography,\u201d Medical Physics, vol. 49, no. 8, pp. 5107\u2013 5120, 2022."},{"location":"denoising_algorithms/#test-set-performance","title":"Test set performance","text":"<p>Below we report the results of the best performing networks of each method on the test dataset. They can be reproduced by running <code>python test.py --print_table</code> (see Test models).</p> Method SSIM (Chest) SSIM (Abdomen) SSIM (Neuro) PSNR (Chest) PSNR (Abdomen) PSNR (Neuro) VIF (Chest) VIF (Abdomen) VIF (Neuro) LD 0.312 0.856 0.914 18.066 29.117 30.923 0.083 0.353 0.578 cnn10 0.559 0.907 0.928 27.307 32.737 31.968 0.175 0.455 0.642 redcnn 0.584 0.913 0.932 28.002 33.685 34.132 0.205 0.504 0.715 qae 0.557 0.903 0.928 27.115 32.304 31.923 0.167 0.424 0.618 wganvgg 0.505 0.893 0.92 25.324 30.906 29.208 0.137 0.39 0.566 resnet 0.581 0.912 0.932 28.032 33.583 33.853 0.21 0.5 0.705 qae 0.557 0.903 0.928 27.115 32.304 31.923 0.167 0.424 0.618 dugan 0.544 0.904 0.93 26.316 32.468 32.078 0.156 0.441 0.656 transct 0.538 0.89 0.893 26.736 30.924 27.363 0.155 0.387 0.461 bilateral 0.529 0.871 0.905 25.057 27.357 29.238 0.143 0.373 0.541"},{"location":"getting_started/","title":"Getting started","text":""},{"location":"getting_started/#installation","title":"Installation","text":"<p>Set up a new environment with Python 3.10 or higher. We recommend using a virtual environment to avoid conflicts with other packages.</p>"},{"location":"getting_started/#from-pypi","title":"From PyPI","text":"<pre><code>pip install ldct-benchmark\n</code></pre>"},{"location":"getting_started/#from-github","title":"From GitHub","text":"<ol> <li>Clone this repository: <code>git clone https://github.com/eeulig/ldct-benchmark</code></li> <li>Install the package with <code>pip install .</code></li> </ol>"},{"location":"getting_started/#in-editable-mode","title":"In editable mode","text":"<p>If you want to implement your own methods or contribute to the project, you should install the package in editable mode using the <code>-e</code>flag:</p> <ol> <li>Clone this repository: <code>git clone https://github.com/eeulig/ldct-benchmark</code></li> <li>Install the package with <code>pip install -e .</code></li> </ol>"},{"location":"getting_started/#dependencies","title":"Dependencies","text":"<ul> <li>We recommend to install the correct PyTorch version for your operating system and CUDA version from PyTorch directly.</li> <li>[Only if you want to use <code>bilateral</code>] Install the trainable bilateral filter by running <code>pip install bilateralfilter_torch</code>. For details on the bilateral filter see the official repository.</li> </ul>"},{"location":"getting_started/#download-the-ldct-data","title":"Download the LDCT data","text":"<p>Info</p> <p>This is only necessary if you want to train or test the models on the LDCT data. If you only want to apply the models to your own data, you can skip this step.</p> <p>Our benchmark uses the Low Dose CT and Projection data<sup>1</sup> which is available from the Cancer Imaging Archive (TCIA). For downloading the data, please follow the instructions below.</p>"},{"location":"getting_started/#1-sign-a-tcia-license-agreement","title":"1. Sign a TCIA license agreement","text":"<p>You must sign and submit a TCIA Restricted License Agreement to download the data. Information on how to do this is provided under \"Data Access\" here.</p>"},{"location":"getting_started/#2-download-the-ldct-data","title":"2. Download the LDCT data","text":"<p>Download Version 3 of the LDCT and Projection Data. We provide the <code>.tcia</code> object containing only the Siemens image-domain data (~27 GB) in <code>assets/manifest.tcia</code>.</p>"},{"location":"getting_started/#using-the-script-recommended","title":"Using the script (recommended)","text":"<p>We provide a script to download all the required data to use the benchmark. Run the following command to download the data to <code>/path/to/datafolder</code>. You must provide your TCIA username and password to access the data:</p> <pre><code>ldctbench-download-data --savedir /path/to/datafolder --username &lt;username&gt; --password &lt;password&gt;\n</code></pre> <p>Info</p> <p>If your username or password contains special characters, you may need to enclose them in single quotes: <pre><code>--username '#!fancy-username' --password 'p@$$w0rd'\n</code></pre></p>"},{"location":"getting_started/#using-the-nbia-data-retriever","title":"Using the NBIA Data Retriever","text":""},{"location":"getting_started/#ubuntu","title":"Ubuntu","text":"<p>After installing the nbia-data-retriever from here, the following command will download the data to <code>/path/to/datafolder</code>:  <pre><code>/opt/nbia-data-retriever/nbia-data-retriever --cli path/to/repo/assets/manifest.tcia -d /path/to/datafolder -v \u2013f -u &lt;username&gt; -p &lt;password&gt;\n</code></pre> with <code>&lt;username&gt;</code> and <code>&lt;password&gt;</code> being your TCIA username/password. Alternatively, you can use the GUI to download using the provided <code>manifest.tcia</code>.</p>"},{"location":"getting_started/#windows-mac","title":"Windows / Mac","text":"<p>After installing the nbia-data-retriever from here use the application to download the data using the provided <code>manifest.tcia</code>. You'll need to provide your TCIA username and password.</p>"},{"location":"getting_started/#3-set-environment-variable-to-the-data-folder","title":"3. Set environment variable to the data folder","text":"<p>For training and testing the models on the LDCT data you need to set the environment variable <code>LDCTBENCH_DATAFOLDER</code> to the path of the downloaded data folder. You can do this by running the following command: <pre><code>export LDCTBENCH_DATAFOLDER=path/to/ldct-data\n</code></pre> where <code>path/to/ldct-data</code> is the path to the downloaded data folder. Alternatively, you can provide the path to the data folder in the <code>config.yaml</code> file for training models or via the argument <code>--datafolder</code> when training/testing models.</p> <ol> <li> <p>McCollough, C., Chen, B., Holmes III, D. R., Duan, X., Yu, Z., Yu, L., Leng, S., &amp; Fletcher, J. (2020). Low Dose CT Image and Projection Data (LDCT-and-Projection-data) (Version 6) [Data set]. The Cancer Imaging Archive. https://doi.org/10.7937/9NPB-2637.\u00a0\u21a9</p> </li> </ol>"},{"location":"model_hub/","title":"Model Hub","text":"<p>We provide pretrained models that can be loaded via <pre><code>from ldctbench.hub import load_model\nnet = load_model(\"&lt;method&gt;\")\n</code></pre> where <code>&lt;method&gt;</code> is one of the method names provided in this table (e.g., <code>cnn10</code>, <code>redcnn</code>, <code>wganvgg</code>, ...) or a member of <code>ldctbench.hub.Methods</code> (e.g., <code>ldctbench.hub.Methods.CNN10</code>). To apply it, make sure that CT images are stored with an offset of 1024, i.e. air has a value of ~24.</p> <pre><code>import numpy as np\nfrom ldctbench.hub import load_model, Methods\nfrom ldctbench.utils import preprocess, denormalize\n\nmethod = Methods.RESNET # method=\"resnet\" also works\n# Setup model\nnet = load_model(method)\n# Define image\nx = # ... some numpy array of shape [1, 512, 512] you wish to denoise\n# Preprocess and normalize input\nx_t = preprocess(x, method=method)\n# Apply network\ny_hat = net(x_t)\n# Denormalize\ny_hat = denormalize(y_hat, method=method)\n</code></pre> <p>We provide a comprehensive example on how to denoise DICOM data using the pretrained models in this example.</p>"},{"location":"examples/denoise_dicoms/","title":"Denoise DICOMs using pretrained models","text":"<p>Prerequisite</p> <p>This example assumes you have the package <code>ldct-benchmark</code> installed. Please refer to Getting Started for instructions on how to do this.</p> <p>The pretrained models provided as part of the model hub can be used to denoise any CT DICOM dataset. See here for a list of all available algorithms.</p> <p>Let's use two of these models, RED-CNN<sup>1</sup> and DU-GAN<sup>2</sup> to denoise DICOM slices from the CT data of the NLM Visible Human Project<sup>3</sup> which can be downloaded from NCI Imaging Data Commons.</p> <p>Warning</p> <p>This is an out-of-distribution setting as data of the Visible Human CT Dataset were acquired with a (31 year old) scanner and scan-protocols that are far from the training data distribution. The results of the models on such data should be interpreted with caution.</p> <p>Start by importing some modules:</p> <pre><code># Make sure that s5cmd is installed! (pip install s5cmd)\nimport os\nimport torch\nfrom ldctbench.hub import Methods\nfrom ldctbench.hub.utils import denoise_dicom\n</code></pre> <p>We'll download 10 slices of the female pelvis data to a new folder <code>./visible-human/orig</code>:</p> <pre><code># Create folders\nfolder = \"./visible-human\"\norig_data = os.path.join(folder, \"orig\")\n\nif not os.path.exists(orig_data):\n    os.makedirs(orig_data)\n\n# Filenames of 10 pelvis slices\nfiles = [\n    \"496788de-f0f0-41fd-b19a-6da82268fd0a.dcm\",\n    \"a535613b-de28-4080-850a-f5647ee33c96.dcm\",\n    \"9f7ef52e-c93d-430a-9038-970a47e95e3a.dcm\",\n    \"0c7ac013-41e3-404f-9081-9e0cc18f4f67.dcm\",\n    \"2591aad8-7673-4a12-98e0-8984dafa5175.dcm\",\n    \"28494e9b-d274-4310-a0ed-15d4220e1dc1.dcm\",\n    \"f5e41514-d30a-4cef-81ac-fce50b4743d8.dcm\",\n    \"21292f8c-072c-4223-859a-1e70bbc87a42.dcm\",\n    \"5a214c6b-6898-43c1-89f3-52c967dff39e.dcm\",\n    \"cd90f914-2b13-4cd9-9119-976a3c5721c1.dcm\",\n]\n\n# Download the data\nfor file in files:\n    os.system(\n        f's5cmd --no-sign-request --endpoint-url https://s3.amazonaws.com cp \"s3://idc-open-data/b9cf8e7a-2505-4137-9ae3-f8d0cf756c13/{file}\" visible-human/orig'\n    )\n</code></pre> <p>The function ldctbench.hub.utils.denoise_dicom can be used to apply a pretrained model either to a single DICOM file or a folder containing multiple DICOM files. The processed DICOMs differ only in the <code>PixelData</code>, all other DICOM tags are identical to those in the source files. We'll use this function to apply the RED-CNN and DU-GAN models to all 10 slices we just downloaded:</p> <p><pre><code># Apply RED-CNN and DU-GAN and store the processed DICOMs\n# to ./visible-human/redcnn and ./visible-human/dugan\nfor method in [Methods.REDCNN, Methods.DUGAN]:\n    denoise_dicom(\n        dicom_path=orig_data,\n        savedir=os.path.join(folder, method.value),\n        method=method,\n        device=torch.device(\"mps\"),  # Use \"mps\" for Apple Silicon, \"cuda\" for NVIDIA GPUs or \"cpu\" for CPU\n    )\n</code></pre> The denoised DICOMs can be loaded with any DICOM viewer. Below we show a comparison of the original and denoised images:</p> Ten slices of the NLM Visible Human Project denoised using RED-CNN and DU-GAN. Original data is courtesy of the U.S. National Library of Medicine. <p>Here we find that both RED-CNN and DU-GAN reduce the noise in the images. Additionally, it can be observed that RED-CNN smooths the images more than DU-GAN does which can be attributed to the fact that DU-GAN is trained in an adversarial fashion, whereas RED-CNN is trained with a simple mean squared error loss.</p> <ol> <li> <p>H. Chen, Y. Zhang, M. K. Kalra, F. Lin, Y. Chen, P. Liao, J. Zhou, and G. Wang, \u201cLow-dose CT with a residual encoder-decoder convolutional neural network,\u201d IEEE Transactions on Medical Imaging, vol. 36, no. 12, pp. 2524\u20132535, Dec. 2017\u00a0\u21a9</p> </li> <li> <p>Z. Huang, J. Zhang, Y. Zhang, and H. Shan, \u201cDU-GAN: Generative adversarial networks with dual-domain U-Net-based discriminators for low-dose CT denoising,\u201d IEEE Transactions on Instrumentation and Measurement, vol. 71, pp. 1\u201312, 2022.\u00a0\u21a9</p> </li> <li> <p>M. J. Ackerman, \"The Visible Human Project,\" in Proceedings of the IEEE, vol. 86, no. 3, pp. 504-511, March 1998, doi: 10.1109/5.662875.\u00a0\u21a9</p> </li> </ol>"},{"location":"examples/hyperparameter_optimization/","title":"Hyperparameter optimization","text":"<p>Prerequisite</p> <p>This example assumes you have:</p> <ol> <li>The package <code>ldct-benchmark</code> installed</li> <li>The LDCT dataset downloaded to a folder <code>path/to/ldct-data</code></li> <li>The environment variable <code>LDCTBENCH_DATAFOLDER</code> set to that folder</li> </ol> <p>Please refer to Getting Started for instructions on how to do these steps.</p>"},{"location":"examples/hyperparameter_optimization/#what-is-hyperparameter-optimization","title":"What is hyperparameter optimization?","text":"<p>Hyperparameter optimization is the process of finding the optimal configuration of hyperparameters for a machine learning model. In deep neural networks, hyperparameters such as learning rate, batch size, or weights of different loss terms can greatly influence model performance. Therefore, proper tuning of these parameters is crucial. The four most common methods for hyperparameter optimization are human optimization, grid search, randomized search, and Bayesian optimization.</p> <ol> <li> <p>Human optimization: Manual fine-tuning involves iteratively adjusting hyperparameters based on intuition, experience, and observation of model performance. While this approach leverages domain expertise, it is time-consuming, subjective, and often fails to explore the hyperparameter space thoroughly.</p> </li> <li> <p>Grid Search: Grid search systematically evaluates every combination of predefined hyperparameter values. It's comprehensive but computationally expensive, especially with many hyperparameters, as the number of combinations grows exponentially.</p> </li> <li> <p>Randomized Search: Randomized search samples hyperparameter values from specified distributions rather than exhaustively trying all combinations. It is generally preferred over grid search<sup>1</sup>.</p> </li> <li> <p>Bayesian Optimization: Bayesian optimization builds a surrogate model of the objective function and uses an acquisition function to determine which hyperparameter combinations to try next. Bayesian optimization has been shown to outperform other optimization methods on a variety of DNN and dataset configurations<sup>2</sup><sup>3</sup>.</p> </li> </ol>"},{"location":"examples/hyperparameter_optimization/#define-hyperparameters-to-optimize","title":"Define hyperparameters to optimize","text":"<p>As for logging, we use Weights &amp; Biases to tune hyperparameters. To configure a hyperparameter optimization run (sweep) you need to define a configuration file in YAML format. The configuration file specifies the optimization method, hyperparameters to tune, and their respective distributions. To optmize the CNN-10 model, we could use the following config file:</p> configs/cnn10-hpopt.yaml <pre><code>project: ldct-benchmark\nprogram: ldctbench-train\ncommand:\n- ${program}\n- ${args_no_boolean_flags}\ndescription: Hyperparameter optimization for CNN-10\nmethod: bayes\nmetric:\n    goal: maximize\n    name: SSIM\nname: cnn10-hpopt\nparameters:\nadam_b1:\n    value: 0.9\nadam_b2:\n    value: 0.999\ndata_norm:\n    value: meanstd\ndata_subset:\n    value: 1\ndevices:\n    value: 0\ndryrun:\n    value: false\niterations_before_val:\n    value: 1000\nlr:\n    distribution: log_uniform_values\n    max: 0.01\n    min: 1e-05\nmax_iterations:\n    distribution: int_uniform\n    max: 100000\n    min: 1000\nmbs:\n    distribution: int_uniform\n    max: 128\n    min: 2\nnum_workers:\n    value: 4\noptimizer:\n    value: adam\npatchsize:\n    distribution: int_uniform\n    max: 128\n    min: 32\nseed:\n    value: 1332\ntrainer:\n    value: cnn10\nvalsamples:\n    value: 8\n</code></pre> <p>For your model, you may need to adjust the hyperparameters and their distributions. All arguments defined in the <code>argparser.py</code> of your method should be set in this configuration file either to a fixed value (not optimized) or a distribution of values (optimized). A full list of hyperparameter configuration options can be found in the W&amp;B documentation.</p>"},{"location":"examples/hyperparameter_optimization/#run-hyperparameter-optimization","title":"Run hyperparameter optimization","text":"<p>Once you defined the configuration file, you can run hyperparameter optimization using the <code>ldctbench-hpopt</code> command. The command takes two arguments: the path to the configuration file and the number of runs to execute. The number of runs determines how many times the optimization algorithm will sample hyperparameters and train the model. The more runs you specify, the more likely you are to find the optimal hyperparameters.</p> <p>To run ten steps of hyperparameter optimization using the configuration file described above, execute the following command:</p> <pre><code>ldctbench-hpopt --config configs/cnn10-hpopt.yaml --n_runs 10\n</code></pre> <p>After all runs are completed, this script reports the best run(1). This run can be used as <code>--methods</code> argument to the test script. You can also visualize the results in the W&amp;B dashboard by logging in to your W&amp;B account and navigating to the respective project/sweep.</p> <ol> <li>What is best is determined from the <code>metric</code> parameter in the config file. For the example above it would be the one that maximizes the SSIM on the validation data.</li> </ol> <ol> <li> <p>Bergstra, James, and Yoshua Bengio. 2012. \u201cRandom Search for Hyper-Parameter Optimization.\u201d Journal of Machine Learning Research 13 (10): 281\u2013305.\u00a0\u21a9</p> </li> <li> <p>Bergstra, James, R\u00e9mi Bardenet, Yoshua Bengio, and Bal\u00e1zs K\u00e9gl. 2011. \u201cAlgorithms for Hyper-Parameter Optimization.\u201d In . Vol. 24.\u00a0\u21a9</p> </li> <li> <p>Snoek, Jasper, Hugo Larochelle, and Ryan P Adams. 2012. \u201cPractical Bayesian Optimization of Machine Learning Algorithms.\u201d In Advances in Neural Information Processing Systems. Vol. 25.\u00a0\u21a9</p> </li> </ol>"},{"location":"examples/ldct_iqa/","title":"Evaluation using LDCT IQA","text":"<p>Prerequisite</p> <p>This example assumes you have:</p> <ol> <li>the package <code>ldct-benchmark</code> installed</li> <li>the LDCT dataset downloaded to a folder <code>path/to/ldct-data</code></li> <li>The environment variable <code>LDCTBENCH_DATAFOLDER</code> set to that folder</li> </ol> <p>Please refer to Getting Started for instructions on how to do these steps.</p> <p>Besides the standard full-reference IQA metrics (SSIM, PSNR, VIF), we also support evaluation using a specialized model for no-reference perceptual image quality assessment of low-dose CT images. This model was the winning entry (agaldran<sup>1</sup>) of the Low-dose Computed Tomography Perceptual Image Quality Assessment Grand Challenge 2023<sup>2</sup> which was organized in conjunction with MICCAI 2023.</p> <p>The aim of the challenge was to develop no-reference IQA methods that correlate well with scores provided by radiologists. To this end, the organizers generated a total of 1500 (1000 train, 200 val, 300 test) images of various quality by introducing noise and streak artifacts into routine-dose abdominal CT images. Resulting images were rated by radiologists on a five-point Likert scale and their mean score was used as the ground truth.</p> <p>The five-point Likert scale was defined as follows (see Table 1 in the paper<sup>2</sup>):</p> Numeric score Verbal descriptive scale Diagnostic quality criteria 0 Bad Desired features are not shown 1 Poor Diagnostic interpretation is impossible 2 Fair Images are suitable for limited clinical interpretation 3 Good Images are suitable for diagnostic interpretation 4 Excellent The anatomical structure is evident <p>Given some CT image, the model predicts a score in the range <code>[0, 4]</code> (with increments of 0.2) on above scale.</p> <p>Use with out-of-distribution (OOD) data</p> <p>Be aware that any evaluation using this model will most likely be an OOD setting and predicted scores should be interpreted with caution. The model was</p> <ul> <li>trained only using abodminal CT images. However, the paper<sup>2</sup> performs some experiments using a clinical head CT dataset, to evaluate the methods generalization capabilities.</li> <li>trained on four distinct noise levels only. These noise levels may not be representative of your data.</li> <li>not trained on denoised images at all. It has only seen routine-dose images and various distorted versions thereof. It remains unclear how well the model generalizes to denoised images.</li> </ul>"},{"location":"examples/ldct_iqa/#evaluate-dicoms-using-the-ldctiqa-model","title":"Evaluate DICOMs using the LDCTIQA model","text":"<p>Let's evaluate a routine-dose and low-dose abdominal CT scan from the LDCT dataset using this LDCTIQA model (see ldctbench.evaluate.ldct_iqa.LDCTIQA for more details).</p> <p><pre><code># Import libraries\nimport os\nfrom importlib.resources import files\n\nimport numpy as np\nimport pydicom\n\nfrom ldctbench.evaluate import LDCTIQA\nfrom ldctbench.utils import load_yaml\n\n# Get abdominal patient from test set\ninfo = load_yaml(files(\"ldctbench.data\").joinpath(\"info.yml\"))\npatient = info[\"test_set\"][5]\n\n# Setup model\nldctiqa = LDCTIQA()\n\n# Evaluation function\ndef evaluate_dicom(folder):\n    res = []\n    for f in os.listdir(folder):\n        file = os.path.join(folder, f)\n        if pydicom.misc.is_dicom(file):\n            ds = pydicom.filereader.dcmread(file)\n            img = ds.pixel_array.astype(\"float32\")\n            score = ldctiqa(img)\n            res.append(score.item())\n    return res\n\n# Evaluate on low-dose and high-dose images\nfor scan, dose in [(\"input\", \"low\"), (\"target\", \"high\")]:\n    scores = evaluate_dicom(\n        folder=os.path.join(os.environ[\"LDCTBENCH_DATAFOLDER\"], patient[scan]),\n    )\n    print(\n        f\"LDCTIQA for {dose}-dose scan of {patient['id']}: {np.mean(scores):.2f} \u00b1 {np.std(scores):.2f}\"\n    )\n</code></pre> Running this script will give us the following scores for the routine-dose and low-dose images of patient <code>L241</code>:</p> Dose level LDCTIQA score Low 2.55 \u00b1 0.44 High 4.0 \u00b1 0 <p>According to our model, this low-dose scan has an image quality between \"Fair\" and \"Good\", while the high-dose scan is rated as \"Excellent\".</p> <p>We also provide a function (ldctbench.evaluate.evaluate_dicom) to evaluate a single DICOM file or a folder of DICOM files and store the results in a <code>json</code> file (if the <code>savedir</code> argument is provided). This function can be used as follows:</p> <p><pre><code>import numpy as np\nfrom ldctbench.evaluate import evaluate_dicom\nscores = evaluate_dicom(dicom_path=\"path/to/dicom/series\", savedir=\"path/to/save\")\nprint(f\"Evaluated {len(scores)} files. LDCTIQA = {np.mean(scores):.2f} \u00b1 {np.std(scores):.2f}\")\n</code></pre> <pre><code>&gt;&gt;&gt; Evaluate DICOM(s): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 137/137 [00:11&lt;00:00, 12.27it/s]\n&gt;&gt;&gt; Saved scores to path/to/save/scores.json\n&gt;&gt;&gt; Evaluated 137 files. LDCTIQA = 3.96 \u00b1 0.08\n</code></pre></p>"},{"location":"examples/ldct_iqa/#evaluate-denoising-methods-using-the-ldctiqa-model","title":"Evaluate denoising methods using the LDCTIQA model","text":"<p>We can also evaluate the performance of denoising methods using the LDCTIQA model. Let's evaluate the performance of the CNN-10 model on the test set of the LDCT dataset.</p> <p>Running <pre><code>ldctbench-test --methods cnn10 --metrics LDCTIQA --print_table\n</code></pre> will give us the following results:</p> Method LDCTIQA (Chest) LDCTIQA (Abdomen) LDCTIQA (Neuro) LD 0 3.208 3.965 cnn10 3.296 3.998 3.972 <p>We find that on all three anatomies, the CNN-10 model improves over the LD baseline in terms of LDCTIQA score and provides a rating between \"Good\" and \"Excellent\" for all anatomies. However, for the neuro scans the improvement is marginal. Note that results on chest and neuro scans should be interpreted with caution, as the model was trained on abdominal CT images only!</p> <ol> <li> <p>Official GitHub repository of agaldran: https://github.com/agaldran/ldct_iqa.\u00a0\u21a9</p> </li> <li> <p>Lee, Wonkyeong, Fabian Wagner, Adrian Galdran, Yongyi Shi, Wenjun Xia, Ge Wang, Xuanqin Mou, et al. 2025. \u201cLow-Dose Computed Tomography Perceptual Image Quality Assessment.\u201d Medical Image Analysis 99 (January):103343. https://doi.org/10.1016/j.media.2024.103343.\u00a0\u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"examples/test_models/","title":"Test models","text":"<p>Prerequisite</p> <p>This example assumes you have:</p> <ol> <li>the package <code>ldct-benchmark</code> installed</li> <li>the LDCT dataset downloaded to a folder <code>path/to/ldct-data</code></li> <li>The environment variable <code>LDCTBENCH_DATAFOLDER</code> set to that folder</li> </ol> <p>Please refer to Getting Started for instructions on how to do these steps.</p>"},{"location":"examples/test_models/#test-script","title":"Test script","text":"<p>Evaluating some model on the test data split of the LDCT data is as simple as running <code>ldctbench-test</code> with the following arguments: <pre><code>usage: ldctbench-test [-h] [--methods METHODS [METHODS ...]] [--metrics METRICS [METRICS ...]] [--datafolder DATAFOLDER] [--data_norm DATA_NORM] [--results_dir RESULTS_DIR] [--device DEVICE] [--print_table]\n\noptions:\n  -h, --help            show this help message and exit\n  --methods METHODS [METHODS ...]\n                        Methods to evaluate. Can be either method name of pretrained networks (cnn10, redcnn, qae, ...), or run name of network trained by the user.\n  --metrics METRICS [METRICS ...]\n                        Metrics to compute. Must be either SSIM, PSNR, RMSE, VIF, or LDCTIQA.\n  --datafolder DATAFOLDER\n                        Path to datafolder. If not set, an environemnt variable LDCTBENCH_DATAFOLDER must be set.\n  --data_norm DATA_NORM\n                        Input normalization: Must be either minmax or meanstd.\n  --results_dir RESULTS_DIR\n                        Folder where to store results.\n  --device DEVICE       CUDA device id.\n  --print_table         Print results in table.\n</code></pre> The argument <code>--datafolder</code> can be omitted if the environment variable <code>LDCTBENCH_DATAFOLDER</code> is set accordingly (see here).</p>"},{"location":"examples/test_models/#evaluate-pretrained-models","title":"Evaluate pretrained models","text":"<p>To test the pretrained CNN-10 and measure SSIM and PSNR, we would run:</p> <p><pre><code>ldctbench-test --methods cnn10 --metrics SSIM PSNR --datafolder path/to/ldct-data\n</code></pre> or  <pre><code>ldctbench-test --methods cnn10 --metrics SSIM PSNR\n</code></pre> if the environment variable <code>LDCTBENCH_DATAFOLDER</code> is set.</p> <p>By default, results are stored in <code>./results/test_metrics.yaml</code>. To print them in a markdown table, we can add <code>--print_table</code>. To reproduce this table, we can run: <pre><code>ldctbench-test --datafolder path/to/ldct-data --print_table\n</code></pre></p>"},{"location":"examples/test_models/#evaluate-custom-trained-models","title":"Evaluate custom trained models","text":"<p>To test a custom trained model we can provide the wandb run name as <code>--method</code> instead if the <code>wandb</code> folder with training logs is in the same directory from which <code>ldctbench-test</code> is called.</p> <p>Evaluating the model trained in the previous example with: <pre><code>ldctbench-test --methods offline-run-&lt;timestamp&gt; --metrics PSNR SSIM --print_table\n</code></pre> will print the following table</p> Method PSNR (Chest) PSNR (Abdomen) PSNR (Neuro) SSIM (Chest) SSIM (Abdomen) SSIM (Neuro) LD 18.066 29.117 30.923 0.312 0.856 0.914 offline-run-&lt;timestamp&gt; 27.036 32.552 32.15 0.552 0.904 0.925 <p>As we can see the simple network improved over the LD baseline on all anatomies and metrics.</p>"},{"location":"examples/train_custom_model/","title":"Train a custom model","text":"<p>Prerequisite</p> <p>This example assumes you have:</p> <ol> <li>the package <code>ldct-benchmark</code> installed in editable mode </li> <li>the LDCT dataset downloaded to a folder <code>path/to/ldct-data</code></li> <li>The environment variable <code>LDCTBENCH_DATAFOLDER</code> set to that folder</li> </ol> <p>Please refer to Getting Started for instructions on how to do these steps.</p>"},{"location":"examples/train_custom_model/#implement-a-custom-method","title":"Implement a custom method","text":"<p>To add a custom method to the benchmark suite we must follow these steps:</p> <ol> <li> <p>Create a folder for the new method in <code>ldctbench/methods</code>. The folder must have the following files</p> <ul> <li><code>__init__.py</code></li> <li><code>argparser.py</code>: Should implement a method <code>add_args()</code> that takes as input an <code>argparse.ArgumentParser</code>, adds custom arguments and returns it.</li> <li><code>network.py</code>: Should implement the model as <code>class Model(torch.nn.Module)</code>.</li> <li><code>Trainer.py</code>: Should imeplement a <code>Trainer</code> class. This class should be initialized with <code>Trainer(args: argparse.Namespace, device: torch.device)</code> and implement a <code>fit()</code> method that trains the network. A base class is provided in <code>methods/base.py</code>.</li> </ul> </li> <li> <p>Add the method to <code>METHODS</code> in <code>ldctbench/utils/argparser.py</code>.</p> </li> </ol> <p>Let's say we want to implement a method called <code>simplecnn</code> which is just a shallow 3-layer CNN. For this, the folder <code>ldctbench/methods/simplecnn</code> should contain the following files:</p> argparser.pynetwork.pyTrainer.pyinit.py <pre><code>from argparse import ArgumentParser\n\n\ndef add_args(parser: ArgumentParser) -&gt; ArgumentParser:\n    parser.add_argument(\n        \"--n_hidden\", type=int, help=\"Number of filters in the hidden layer of the CNN\"\n    )\n    return parser\n</code></pre> <pre><code>from argparse import Namespace\n\nimport torch.nn as nn\n\n\nclass Model(nn.Module):\n    \"\"\"An simple network Conv -&gt; ReLU -&gt; Conv -&gt; ReLU -&gt; Conv\"\"\"\n\n    def __init__(self, args: Namespace):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(1, args.n_hidden, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(args.n_hidden, args.n_hidden, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(args.n_hidden, 1, 3, padding=1),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n</code></pre> <pre><code>from argparse import Namespace\n\nimport torch\nimport torch.nn as nn\n\nfrom ldctbench.methods.base import BaseTrainer\nfrom ldctbench.utils.training_utils import setup_optimizer\n\nfrom .network import Model\n\n\nclass Trainer(BaseTrainer):\n    \"\"\"Trainer class for a simple CNN\"\"\"\n\n    def __init__(self, args: Namespace, device: torch.device):\n        \"\"\"Init function\n\n        Parameters\n        ----------\n        args : Namespace\n            Arguments to configure the trainer.\n        device : torch.device\n            Torch device to use for training.\n        \"\"\"\n        super().__init__(args, device)\n        self.criterion = nn.MSELoss()\n        self.model = Model(args).to(self.dev)\n        if isinstance(self.args.devices, list):\n            self.model = nn.DataParallel(self.model, device_ids=self.args.devices)\n        self.optimizer = setup_optimizer(args, self.model.parameters())\n</code></pre> <pre><code># An empty file\n</code></pre> <p>Additionally, we have to change <code>METHODS</code> in <code>ldctbench/utils/argparser.py</code> to include the new method:</p> ldctbench/utils/argparser.py <pre><code>...\nMETHODS = [\n    \"bilateral\",\n    \"cnn10\",\n    \"dugan\",\n    \"qae\",\n    \"redcnn\",\n    \"resnet\",\n    \"transct\",\n    \"wganvgg\",\n    \"simplecnn\"  # &lt;-- added here\n]\n...\n</code></pre>"},{"location":"examples/train_custom_model/#train-a-method","title":"Train a method","text":"<p>You can train the method using <pre><code>ldctbench-train --arg1 val1 --arg2 val2\n</code></pre> or, alternatively <pre><code>ldctbench-train --config configs/&lt;method&gt;.yaml\n</code></pre> if a <code>.yaml</code> file containing all necessary arguments to run the method is provided in <code>configs/</code>.</p> <p>Let us now train the <code>simplecnn</code> method with the following config file that we place in <code>configs/</code></p> configs/simplecnn.yaml <pre><code>optimizer: adam  # Which optimizer to use\nadam_b1: 0.9  # Adam's beta1\nadam_b2: 0.999  # Adam's beta2\ncuda: true  # Use CUDA\ndata_norm: meanstd  # Data normalization\ndata_subset: 1.  # Fraction of the dataset to use (1.0 = all data)\ndevices: 0  # Which GPU to use\ndryrun: true  # Do not sync results to wandb\neval_patchsize: 128  # Patchsize for evaluation\niterations_before_val: 1000  # Number of iterations before validation\nlr: 0.0001  # Learning rate\nmax_iterations: 50000 # Maximum number of iterations \nmbs: 64 # Mini-batch size\nnum_workers: 8 # Number of workers for data loading\npatchsize: 64  # Patchsize for training\nseed: 1332  # Random seed\ntrainer: simplecnn  # Our new method\nvalsamples: 8  # Number of validation samples to log\nn_hidden: 64  # Number of filters in the hidden layer of the CNN\n</code></pre> <p>Training the method is then done by running <pre><code>ldctbench-train --config configs/simplecnn.yaml\n</code></pre> and should take approximately 25 minutes on a single GPU. The training logs are stored to a folder <code>./wandb/offline-run-&lt;timestamp&gt;/files</code> (relative to the folder from which <code>ldctbench-train</code> was called). Let's have a look at the plot of training and validation loss that we find in that folder:</p> Training and validation loss for the 'simplecnn' method"},{"location":"reference/ldctbench/data/LDCTMayo/","title":"ldctbench.data.LDCTMayo","text":""},{"location":"reference/ldctbench/data/LDCTMayo/#ldctbench.data.LDCTMayo.LDCTMayo","title":"<code>LDCTMayo(mode, args)</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset class for the LDCT dataset</p> <p>Parameters:</p> <ul> <li> <code>mode</code>               (<code>str</code>)           \u2013            <p>Which subset to use. Must be <code>train</code>, <code>val</code>, or <code>test</code>.</p> </li> <li> <code>args</code>               (<code>Namespace</code>)           \u2013            <p>Command line argumetns passed to the dataset class.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>seed</code>               (<code>int</code>)           \u2013            <p>Random seed to use</p> </li> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>Root path to datafolder</p> </li> <li> <code>patchsize</code>               (<code>int</code>)           \u2013            <p>Patchsize to use for training.</p> </li> <li> <code>eval_patchsize</code>               (<code>int</code>)           \u2013            <p>Patchsize to use for validation.</p> </li> <li> <code>data_subset</code>               (<code>float</code>)           \u2013            <p>Subset of the data to use.</p> </li> <li> <code>data_norm</code>               (<code>str</code>)           \u2013            <p>Normalization of the data, must be <code>meanstd</code> or <code>minmax</code>.</p> </li> <li> <code>info</code>               (<code>dict</code>)           \u2013            <p>Dictionary of <code>info.yml</code> associated with the dataset.</p> </li> <li> <code>samples</code>               (<code>list</code>)           \u2013            <p>List of all image slices</p> </li> <li> <code>weights</code>               (<code>list</code>)           \u2013            <p>List of weights associated with each slice. Slices of each patient having <code>n_slices</code> are weighted with <code>1/n_slices</code>.</p> </li> </ul> <p>Examples:</p> <p>Create a training dataset</p> <pre><code>&gt;&gt;&gt; from argparse import Namespace\n&gt;&gt;&gt; from ldctbench.data import LDCTMayo\n&gt;&gt;&gt; args = Namespace(seed=1332, path='/path/to/dataset/root', data_norm='meanstd', data_subset=1.0, patchsize=128)\n&gt;&gt;&gt; train_data = LDCTMayo('train', args)\n</code></pre> <p>Create a validation dataset</p> <pre><code>&gt;&gt;&gt; from argparse import Namespace\n&gt;&gt;&gt; from ldctbench.data import LDCTMayo\n&gt;&gt;&gt; args = Namespace(seed=1332, path='/path/to/dataset/root', data_norm='meanstd', data_subset=1.0, eval_patchsize=128)\n&gt;&gt;&gt; train_data = LDCTMayo('val', args)\n</code></pre>"},{"location":"reference/ldctbench/data/LDCTMayo/#ldctbench.data.LDCTMayo.LDCTMayo.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Given an index, get slice, perform random cropping, normalize and return</p>"},{"location":"reference/ldctbench/data/LDCTMayo/#ldctbench.data.LDCTMayo.LDCTMayo.__len__","title":"<code>__len__()</code>","text":"<p>Length of dataset</p>"},{"location":"reference/ldctbench/data/LDCTMayo/#ldctbench.data.LDCTMayo.LDCTMayo._idx2filename","title":"<code>_idx2filename(idx, n_slices)</code>  <code>staticmethod</code>","text":"<p>Get filename for a patient of LDCT data given slice and number of slices in the scan.</p> <p>Parameters:</p> <ul> <li> <code>idx</code>               (<code>int</code>)           \u2013            <p>Slice idx for which to return filename</p> </li> <li> <code>n_slices</code>               (<code>int</code>)           \u2013            <p>Number of slices of the scan necessary to figure out number of trailing zeros</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Filename</p> </li> </ul>"},{"location":"reference/ldctbench/data/LDCTMayo/#ldctbench.data.LDCTMayo.LDCTMayo._normalize","title":"<code>_normalize(X)</code>","text":"<p>Normalize samples with precomputed mean/std or min/max</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray</code>)           \u2013            <p>Array to normalize</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>Normalized array</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If normalization method <code>self.data_norm</code> is neither meanstd nor minmax</p> </li> </ul>"},{"location":"reference/ldctbench/data/LDCTMayo/#ldctbench.data.LDCTMayo.LDCTMayo._random_crop","title":"<code>_random_crop(images)</code>","text":"<p>Randomly crop the same patch from images (e.g. ground truth and input)</p> <p>Parameters:</p> <ul> <li> <code>images</code>               (<code>List[ndarray]</code>)           \u2013            <p>List of images to crop in same position for</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[ndarray]</code>           \u2013            <p>List of cropped images</p> </li> </ul>"},{"location":"reference/ldctbench/data/LDCTMayo/#ldctbench.data.LDCTMayo.LDCTMayo.denormalize","title":"<code>denormalize(X)</code>","text":"<p>Denormalize samples with precomputed mean/std or min/max</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray</code>)           \u2013            <p>Array to denormalize</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>Denormalized array</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If normalization method <code>self.data_norm</code> is neihter meanstd nor minmax</p> </li> </ul>"},{"location":"reference/ldctbench/data/LDCTMayo/#ldctbench.data.LDCTMayo.LDCTMayo.reset_seed","title":"<code>reset_seed()</code>","text":"<p>Reset random seeds</p>"},{"location":"reference/ldctbench/data/LDCTMayo/#ldctbench.data.LDCTMayo.LDCTMayo.to_torch","title":"<code>to_torch(X)</code>  <code>staticmethod</code>","text":"<p>Convert input image to torch tensor and unsqueeze</p>"},{"location":"reference/ldctbench/data/prepare_dataset/","title":"ldctbench.data.prepare_dataset","text":""},{"location":"reference/ldctbench/data/prepare_dataset/#ldctbench.data.prepare_dataset.generate_tvt_subsets","title":"<code>generate_tvt_subsets(data, tvt_split=(0.7, 0.2, 0.1))</code>","text":"<p>Arguments:     data:   Amount of samples (int) or list of samles (list)     tvt_split:    Split ratio for training and validation</p> <p>Returns:     Disjoint subsets of the set of all samples. If data is an int, we return indices, else sample sets</p>"},{"location":"reference/ldctbench/evaluate/ldct_iqa/","title":"ldctbench.evaluate.ldct_iqa","text":""},{"location":"reference/ldctbench/evaluate/ldct_iqa/#ldctbench.evaluate.ldct_iqa.LDCTIQA","title":"<code>LDCTIQA(device=None)</code>","text":"<p>Class to perform no-reference IQA on LDCT images using the winning model of the Low-dose Computed Tomography Perceptual Image Quality Assessment Grand Challenge 2023<sup>1</sup> which was organized in conjunction with MICCAI 2023.</p> <p>The aim of the challenge was to develop no-reference IQA methods that correlate well with scores provided by radiologists. To this end, the organizers generated a total of 1,500 (1,000 train, 200 val, 300 test) images of various quality by introducing noise and streak artifacts into routine-dose abdominal CT images. Resulting images were rated by radiologists on a five-point Likert scale and their mean score was used as the ground truth.</p> <p>The five-point Likert scale was defined as follows (see Table 1 in the paper<sup>1</sup>):</p> Numeric score Verbal descriptive scale Diagnostic quality criteria 0 Bad Desired features are not shown 1 Poor Diagnostic interpretation is impossible 2 Fair Images are suitable for limited clinical interpretation 3 Good Images are suitable for diagnostic interpretation 4 Excellent The anatomical structure is evident <p>The model that we use here is a slight variation of the winning model (agaldran). The differences to the model used in the challenge are:</p> <ul> <li>We retrained including the additional 300 test images from the challenge</li> <li>Only using the muli-head swin transformer (no ResNeXt)</li> <li>No ensemble, only one model on a single training/validation split of the 1,500 images</li> </ul> <p>Use with out-of-distribution (OOD) data</p> <p>Be aware that any evaluation using this model will most likely be an OOD setting and predicted scores should be interpreted with caution. The model was</p> <ul> <li>trained only using abodminal CT images. However, the paper<sup>1</sup> performs some experiments using a clinical head CT dataset, to evaluate the methods generalization capabilities.</li> <li>trained on four distinct noise levels only. These noise levels may not be representative of your data.</li> <li>not trained on denoised images at all. It has only seen routine-dose images and various distorted versions thereof. It remains unclear how well the model generalizes to denoised images.</li> </ul> <ol> <li> <p>Lee, Wonkyeong, Fabian Wagner, Adrian Galdran, Yongyi Shi, Wenjun Xia, Ge Wang, Xuanqin Mou, et al. 2025. \u201cLow-Dose Computed Tomography Perceptual Image Quality Assessment.\u201d Medical Image Analysis 99 (January):103343. https://doi.org/10.1016/j.media.2024.103343.\u00a0\u21a9\u21a9\u21a9</p> </li> </ol> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>device</code>, default:                   <code>None</code> )           \u2013            <p>Device to run LDCTIQA model on</p> </li> </ul>"},{"location":"reference/ldctbench/evaluate/ldct_iqa/#ldctbench.evaluate.ldct_iqa.LDCTIQA.__call__","title":"<code>__call__(x, preprocess=True)</code>","text":"<p>Predict the IQA score for a given image</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Union[Tensor, ndarray]</code>)           \u2013            <p>if <code>preprocess</code> is <code>True</code>, expects numpy float array in HU + 1024 (i.e., air should have a value of ~24) of shape <code>[1,512,512]</code>. Otherwise, expects preprocessed torch tensor of shape <code>[B,3,512,512]</code></p> </li> <li> <code>preprocess</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether inputs should be preprocessed (i.e., windowed and normalized)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Predicted score on a five-point Likert scale <code>[0,4]</code> in <code>0.2</code> increments</p> </li> </ul> <p>Examples:</p> <p>Evaluate IQA on a single DICOM slice</p> <pre><code>&gt;&gt;&gt; import pydicom\n&gt;&gt;&gt; ds = pydicom.filereader.dcmread(\"path/to/dicom\")\n&gt;&gt;&gt; img = ds.pixel_array.astype(\"float32\")\n&gt;&gt;&gt; # Ensure that image has expected offset and scaling\n&gt;&gt;&gt; assert ds.RescaleSlope == \"1\" and ds.RescaleIntercept == \"-1024\"\n&gt;&gt;&gt; iqa = LDCTIQA()\n&gt;&gt;&gt; score = iqa(img)\n</code></pre>"},{"location":"reference/ldctbench/evaluate/ldct_iqa/#ldctbench.evaluate.ldct_iqa.LDCTIQA.preprocess","title":"<code>preprocess(x)</code>","text":"<p>Preprocess a given CT image</p> <p>The function takes a numpy float array in HU + 1024, applies windowing with (C,W) = (300, 1400) HU (as was done for the training data) and normalizes the image to the ImageNet mean and standard deviation.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>ndarray</code>)           \u2013            <p>A <code>np.ndarray</code> of shape <code>[512,512]</code> or <code>[1,512,512]</code> representing a CT image in HU + 1024</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If input is not a numpy array</p> </li> <li> <code>ValueError</code>             \u2013            <p>If image shape is not <code>[512,512]</code> or <code>[1,512,512]</code></p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Preprocessed image as torch.Tensor of shape <code>[1,3,512,512]</code></p> </li> </ul>"},{"location":"reference/ldctbench/evaluate/ldct_iqa/#ldctbench.evaluate.ldct_iqa.evaluate_dicom","title":"<code>evaluate_dicom(dicom_path, savedir=None, device=None, disable_progress=False)</code>","text":"<p>Evaluate LDCTIQA on a single DICOM file or a series of DICOM files in a folder</p> <p>Parameters:</p> <ul> <li> <code>dicom_path</code>               (<code>str</code>)           \u2013            <p>Path to a single DICOM file or a folder containig one or more DICOM files</p> </li> <li> <code>savedir</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Foldername where to store evaluation results. If not provided, results are not saved, only returned</p> </li> <li> <code>device</code>               (<code>Optional[device]</code>, default:                   <code>None</code> )           \u2013            <p>torch.device, optional</p> </li> <li> <code>disable_progress</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Disable progress bar, by default False</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code>           \u2013            <p>List of IQA scores for each DICOM file</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If provided path is neither a DICOM file nor a folder containing at least one DICOM file</p> </li> <li> <code>ValueError</code>             \u2013            <p>If image shape is not <code>512 x 512</code></p> </li> </ul> <p>Examples:</p> <p>Evaluate a folder of DICOM files and save the results to a file <code>scores.json</code> in the provided <code>savedir</code>: <pre><code>from ldctbench.evaluate import evaluate_dicom\nscores = evaluate_dicom(dicom_path=\"path/to/dicom/series\", savedir=\"path/to/save\")\n</code></pre> Evalauate a folder of DICOM files and return scores (don't save them): <pre><code>from ldctbench.evaluate import evaluate_dicom\nscores = evaluate_dicom(dicom_path=\"path/to/dicom/series\")\n</code></pre></p> <p>The function is also documented in this example.</p>"},{"location":"reference/ldctbench/evaluate/utils/","title":"ldctbench.evaluate.utils","text":""},{"location":"reference/ldctbench/evaluate/utils/#ldctbench.evaluate.utils.compute_metric","title":"<code>compute_metric(targets, predictions, metrics, denormalize_fn=None, exam_type=None, ldct_iqa=None)</code>","text":"<p>Compute metrics for given predictions and targets</p> <p>Parameters:</p> <ul> <li> <code>targets</code>               (<code>Union[Tensor, ndarray]</code>)           \u2013            <p>Tensor or ndarray of shape (mbs, 1, H, W) holding ground truth</p> </li> <li> <code>predictions</code>               (<code>Union[Tensor, ndarray]</code>)           \u2013            <p>Tensor or ndarray of shape (mbs, 1, H, W) holding predictions</p> </li> <li> <code>metrics</code>               (<code>List[str]</code>)           \u2013            <p>List of metrics must be \"VIF\" | \"PSNR\" | \"RMSE\" | \"SSIM\" | \"LDCTIQA\"</p> </li> <li> <code>denormalize_fn</code>               (<code>Optional[Callable]</code>, default:                   <code>None</code> )           \u2013            <p>Function to use for denormalizing, by default None</p> </li> <li> <code>exam_type</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Exam type (for computing SSIM and PSNR on windowed images), by default None</p> </li> <li> <code>ldct_iqa</code>               (<code>Optional[LDCTIQA]</code>, default:                   <code>None</code> )           \u2013            <p>LDCTIQA object for computing LDCTIQA score, by default None</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict[str, List]</code>           \u2013            <p>Dictionary containing for each metric a list of len(mbs)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>predictions.shape != targets.shape</code></p> </li> <li> <code>ValueError</code>             \u2013            <p>If element in metric is not \"VIF\" | \"PSNR\" | \"RMSE\" | \"SSIM\" | \"LDCTIQA\"</p> </li> </ul>"},{"location":"reference/ldctbench/evaluate/utils/#ldctbench.evaluate.utils.denormalize","title":"<code>denormalize(x, method=None, normalization=None)</code>","text":"<p>Denormalize tensor or ndarray based on normalization type of trained model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Union[Tensor, ndarray]</code>)           \u2013            <p>Tensor or ndarray to normalize</p> </li> <li> <code>method</code>               (<code>Union[Literal[RESNET, CNN10, DUGAN, QAE, REDCNN, TRANSCT, WGANVGG, BILATERAL], str, None]</code>, default:                   <code>None</code> )           \u2013            <p>Enum item or string, specifying model to dermine which normalization to use. See ldctbench.hub.methods.Methods for more info, by default None</p> </li> <li> <code>normalization</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Normalization method, must be meanstd | minmax, by default None</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[Tensor, ndarray]</code>           \u2013            <p>Normalized tensor or ndarray</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>normalization</code> is neither \"meanstd\" nor \"minmax\"</p> </li> </ul>"},{"location":"reference/ldctbench/evaluate/utils/#ldctbench.evaluate.utils.normalize","title":"<code>normalize(x, method=None, normalization=None)</code>","text":"<p>Normalize tensor or ndarray based on normalization type of trained model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Union[Tensor, ndarray]</code>)           \u2013            <p>Tensor or ndarray to normalize</p> </li> <li> <code>method</code>               (<code>Union[Literal[RESNET, CNN10, DUGAN, QAE, REDCNN, TRANSCT, WGANVGG, BILATERAL], str, None]</code>, default:                   <code>None</code> )           \u2013            <p>Enum item or string, specifying model to dermine which normalization to use. See ldctbench.hub.methods.Methods for more info, by default None</p> </li> <li> <code>normalization</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Normalization method, must be meanstd | minmax, by default None</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[Tensor, ndarray]</code>           \u2013            <p>Normalized tensor or ndarray</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>normalization</code> is neither \"meanstd\" nor \"minmax\"</p> </li> </ul>"},{"location":"reference/ldctbench/evaluate/utils/#ldctbench.evaluate.utils.preprocess","title":"<code>preprocess(x, **normalization_kwargs)</code>","text":"<p>Preprocess input tensor</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Union[Tensor, ndarray]</code>)           \u2013            <p>Input tensor or ndarray</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Preprocessed tensor</p> </li> </ul>"},{"location":"reference/ldctbench/evaluate/utils/#ldctbench.evaluate.utils.setup_trained_model","title":"<code>setup_trained_model(run_name, device=torch.device('cuda'), network_name='Model', state_dict=None, return_args=False, return_model=True, eval=True, **model_kwargs)</code>","text":"<p>Setup a trained model with run in <code>./wandb</code></p> <p>Parameters:</p> <ul> <li> <code>run_name</code>               (<code>str</code>)           \u2013            <p>Name of run (is the same as foldername)</p> </li> <li> <code>device</code>               (<code>device</code>, default:                   <code>device('cuda')</code> )           \u2013            <p>Device to move model to, by default torch.device(\"cuda\")</p> </li> <li> <code>network_name</code>               (<code>str</code>, default:                   <code>'Model'</code> )           \u2013            <p>Class name of network, by default \"Model\"</p> </li> <li> <code>state_dict</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Name of state_dict. If None, model is initialized with random parameters, by default None</p> </li> <li> <code>return_args</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Return args of training run, by default False</p> </li> <li> <code>return_model</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Return model, by default True</p> </li> <li> <code>eval</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Set model to eval mode, by default True</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[Tuple[Module, Namespace], Module, Namespace]</code>           \u2013            <p>Either model, args or model and args of training run</p> </li> </ul>"},{"location":"reference/ldctbench/evaluate/utils/#ldctbench.evaluate.utils.vif","title":"<code>vif(x, y, sigma_n_sq=2.0)</code>","text":"<p>Compute visual information fidelity</p>"},{"location":"reference/ldctbench/hub/load_model/","title":"ldctbench.hub.load_model","text":""},{"location":"reference/ldctbench/hub/load_model/#ldctbench.hub.load_model.load_model","title":"<code>load_model(method, eval=True, device=None)</code>","text":"<p>Load a pretrained model</p> <p>Parameters:</p> <ul> <li> <code>method</code>               (<code>Union[Literal[RESNET, CNN10, DUGAN, QAE, REDCNN, TRANSCT, WGANVGG, BILATERAL], str]</code>)           \u2013            <p>Enum item or string, specifying model to load. See ldctbench.hub.methods.Methods for more info.</p> </li> <li> <code>eval</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Return network in eval mode, by default True</p> </li> <li> <code>device</code>               (<code>Optional[device]</code>, default:                   <code>None</code> )           \u2013            </li> </ul> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>The pretrained model</p> </li> </ul> <p>Examples:</p> <p>Load a pretrained resnet model for LDCT denoising:</p> <pre><code>&gt;&gt;&gt; from ldctbench.hub import load_model, Methods\n&gt;&gt;&gt; net = load_model(Methods.RESNET)  # is the same as load_model(\"resnet\")\n</code></pre>"},{"location":"reference/ldctbench/hub/methods/","title":"ldctbench.hub.methods","text":""},{"location":"reference/ldctbench/hub/methods/#ldctbench.hub.methods.Methods","title":"<code>Methods</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum of available Methods.</p> <p>Can be used as <code>method</code> argument in ldctbench.hub.load_model.load_model. Alternatively, their string representations may be used.</p> <p>Attributes:</p> <ul> <li> <code>CNN10</code>               (<code>str</code>)           \u2013            <p>Has string representation <code>\"cnn10\"</code></p> </li> <li> <code>REDCNN</code>               (<code>str</code>)           \u2013            <p>Has string representation <code>\"redcnn\"</code></p> </li> <li> <code>WGANVGG</code>               (<code>str</code>)           \u2013            <p>Has string representation <code>\"wganvgg\"</code></p> </li> <li> <code>RESNET</code>               (<code>str</code>)           \u2013            <p>Has string representation <code>\"resnet\"</code></p> </li> <li> <code>QAE</code>               (<code>str</code>)           \u2013            <p>Has string representation <code>\"qae\"</code></p> </li> <li> <code>DUGAN</code>               (<code>str</code>)           \u2013            <p>Has string representation <code>\"dugan\"</code></p> </li> <li> <code>TRANSCT</code>               (<code>str</code>)           \u2013            <p>Has string representation <code>\"transct\"</code></p> </li> <li> <code>BILATERAL</code>               (<code>str</code>)           \u2013            <p>Has string representation <code>\"bilateral\"</code></p> </li> </ul>"},{"location":"reference/ldctbench/hub/utils/","title":"ldctbench.hub.utils","text":""},{"location":"reference/ldctbench/hub/utils/#ldctbench.hub.utils.denoise_dicom","title":"<code>denoise_dicom(dicom_path, savedir, method, device=None, disable_progress=False)</code>","text":"<p>Denoise a single DICOM or multiple DICOMs in a folder with a model from the model hub</p> <p>Parameters:</p> <ul> <li> <code>dicom_path</code>               (<code>str</code>)           \u2013            <p>Path to a single DICOM file or a folder containig one or more DICOM files</p> </li> <li> <code>savedir</code>               (<code>str</code>)           \u2013            <p>Foldername where to store denoised DICOM file(s) in</p> </li> <li> <code>method</code>               (<code>Union[Literal[RESNET, CNN10, DUGAN, QAE, REDCNN, TRANSCT, WGANVGG, BILATERAL], str]</code>)           \u2013            <p>Enum item or string, specifying model to load. See ldctbench.hub.methods.Methods for more info.</p> </li> <li> <code>device</code>               (<code>Optional[device]</code>, default:                   <code>None</code> )           \u2013            <p>torch.device, optional</p> </li> <li> <code>disable_progress</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Disable progress bar (if denoising multiple DICOM files), by default False</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If provided path is neither a DICOM file nor a folder containing at least one DICOM file</p> </li> </ul> <p>Examples:</p> <p>A comprehensive example using this function is provided in [Getting Started][denoise-dicom-using-pretrained-models].</p>"},{"location":"reference/ldctbench/hub/utils/#ldctbench.hub.utils.denoise_numpy","title":"<code>denoise_numpy(x, method, device=None, num_slices_parallel=6, is_normalized=False)</code>","text":"<p>Denoise a numpy array with a model from the model hub</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>ndarray</code>)           \u2013            <p>Input numpy array. Must be either 2d (single slice) or 3d (with slices-first) order</p> </li> <li> <code>method</code>               (<code>Union[Literal[RESNET, CNN10, DUGAN, QAE, REDCNN, TRANSCT, WGANVGG, BILATERAL], str, Module]</code>)           \u2013            <p>Enum item or string, specifying model to load. See ldctbench.hub.methods.Methods for more info. You can also pass a nn.Module directly.</p> </li> <li> <code>device</code>               (<code>Optional[device]</code>, default:                   <code>None</code> )           \u2013            <p>torch.device, optional</p> </li> <li> <code>num_slices_parallel</code>               (<code>int</code>, default:                   <code>6</code> )           \u2013            <p>Number of slices to process in parallel, by default 6</p> </li> <li> <code>is_normalized</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether data in array is already normalized for the model, by default False</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>Prediction of the network</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If input is not a 2d or 3d array</p> </li> </ul>"},{"location":"reference/ldctbench/methods/base/","title":"ldctbench.methods.base","text":""},{"location":"reference/ldctbench/methods/bilateral/Trainer/","title":"ldctbench.methods.bilateral.Trainer","text":""},{"location":"reference/ldctbench/methods/bilateral/Trainer/#ldctbench.methods.bilateral.Trainer.Trainer","title":"<code>Trainer(args, device)</code>","text":"<p>               Bases: <code>BaseTrainer</code></p> <p>Trainer class for trainable bilateral filter<sup>1</sup></p> <ol> <li> <p>F. Wagner et al., \u201cUltralow-parameter denoising: Trainable bilateral filter layers in computed tomography,\u201d Medical Physics, vol. 49, no. 8, pp. 5107\u20135120, 2022.\u00a0\u21a9</p> </li> </ol> <p>Parameters:</p> <ul> <li> <code>args</code>               (<code>Namespace</code>)           \u2013            <p>Arguments to configure the trainer.</p> </li> <li> <code>device</code>               (<code>device</code>)           \u2013            <p>Torch device to use for training.</p> </li> </ul>"},{"location":"reference/ldctbench/methods/bilateral/argparser/","title":"ldctbench.methods.bilateral.argparser","text":""},{"location":"reference/ldctbench/methods/bilateral/network/","title":"ldctbench.methods.bilateral.network","text":""},{"location":"reference/ldctbench/methods/bilateral/network/#ldctbench.methods.bilateral.network.Model","title":"<code>Model(args, sigma_xyz_init=0.5, sigma_r_init=0.01)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Implements a model with trainable, bilateral filters</p> <p>Parameters:</p> <ul> <li> <code>args</code>               (<code>Namespace</code>)           \u2013            <p>Arguments passed to the model</p> </li> <li> <code>sigma_xyz_init</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Initial value for sigma_xyz, by default 0.5</p> </li> <li> <code>sigma_r_init</code>               (<code>float</code>, default:                   <code>0.01</code> )           \u2013            <p>Initial value for sigma_r, by default 0.01</p> </li> </ul>"},{"location":"reference/ldctbench/methods/cnn10/Trainer/","title":"ldctbench.methods.cnn10.Trainer","text":""},{"location":"reference/ldctbench/methods/cnn10/Trainer/#ldctbench.methods.cnn10.Trainer.Trainer","title":"<code>Trainer(args, device)</code>","text":"<p>               Bases: <code>BaseTrainer</code></p> <p>Trainer class for CNN10<sup>1</sup></p> <ol> <li> <p>H. Chen et al., \u201cLow-dose CT via convolutional neural network,\u201d Biomed Opt Express, vol. 8, no. 2, pp. 679\u2013694, Jan. 2017.\u00a0\u21a9</p> </li> </ol> <p>Parameters:</p> <ul> <li> <code>args</code>               (<code>Namespace</code>)           \u2013            <p>Arguments to configure the trainer.</p> </li> <li> <code>device</code>               (<code>device</code>)           \u2013            <p>Torch device to use for training.</p> </li> </ul>"},{"location":"reference/ldctbench/methods/cnn10/argparser/","title":"ldctbench.methods.cnn10.argparser","text":""},{"location":"reference/ldctbench/methods/cnn10/network/","title":"ldctbench.methods.cnn10.network","text":""},{"location":"reference/ldctbench/methods/cnn10/network/#ldctbench.methods.cnn10.network.Model","title":"<code>Model(args)</code>","text":"<p>               Bases: <code>Module</code></p> <p>CNN-10 network. As described in the original paper, we use a simple 3 layer CNN with following parameters</p> <pre><code>Layer 1: Kernel size 9x9, number of filters 64\nLayer 2: Kernel size 3x3, number of filters 32\nLayer 3: Kernel size 5x5, number of filters 1.\n</code></pre> <p>They use ReLU as nonlinearity and no BatchNorm.</p>"},{"location":"reference/ldctbench/methods/dugan/Trainer/","title":"ldctbench.methods.dugan.Trainer","text":""},{"location":"reference/ldctbench/methods/dugan/Trainer/#ldctbench.methods.dugan.Trainer.Trainer","title":"<code>Trainer(args, device)</code>","text":"<p>               Bases: <code>BaseTrainer</code></p> <p>Trainer for DUGAN<sup>1</sup></p> <ol> <li> <p>Z. Huang, J. Zhang, Y. Zhang, and H. Shan, \u201cDU-GAN: Generative adversarial networks with dual-domain U-Net-based discriminators for low-dose CT denoising,\u201d IEEE Transactions on Instrumentation and Measurement, vol. 71, pp. 1\u201312, 2022.\u00a0\u21a9</p> </li> </ol> <p>Parameters:</p> <ul> <li> <code>args</code>               (<code>Namespace</code>)           \u2013            <p>Arguments to configure the trainer.</p> </li> <li> <code>device</code>               (<code>device</code>)           \u2013            <p>Torch device to use for training.</p> </li> </ul>"},{"location":"reference/ldctbench/methods/dugan/argparser/","title":"ldctbench.methods.dugan.argparser","text":""},{"location":"reference/ldctbench/methods/dugan/network/","title":"ldctbench.methods.dugan.network","text":""},{"location":"reference/ldctbench/methods/dugan/network/#ldctbench.methods.dugan.network.UNet","title":"<code>UNet(repeat_num, use_tanh=False, use_sigmoid=False, skip_connection=True, use_discriminator=True, conv_dim=64, in_channels=1)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Parameters:</p> <ul> <li> <code>repeat_num</code>               (<code>int</code>)           \u2013            <p>Number of downsampling steps</p> </li> <li> <code>use_tanh</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Use tanh as final nonlinearity, by default False</p> </li> <li> <code>use_sigmoid</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Use sigmoid as final nonlinearity, by default False</p> </li> <li> <code>skip_connection</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Use skip connection, by default True</p> </li> <li> <code>use_discriminator</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Use encoder (at bottleneck) as additional output, by default True</p> </li> <li> <code>conv_dim</code>               (<code>int</code>, default:                   <code>64</code> )           \u2013            <p>Number of features in first layer, by default 64</p> </li> <li> <code>in_channels</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of channels of first layer, by default 1</p> </li> </ul>"},{"location":"reference/ldctbench/methods/dugan/utils/","title":"ldctbench.methods.dugan.utils","text":""},{"location":"reference/ldctbench/methods/qae/Trainer/","title":"ldctbench.methods.qae.Trainer","text":""},{"location":"reference/ldctbench/methods/qae/Trainer/#ldctbench.methods.qae.Trainer.Trainer","title":"<code>Trainer(args, device)</code>","text":"<p>               Bases: <code>BaseTrainer</code></p> <p>Trainer class for Quadratic Autoencoder (QAE)<sup>1</sup>.</p> <ol> <li> <p>F. Fan et al., \u201cQuadratic autoencoder (Q-AE) for low-dose CT denoising,\u201d IEEE Transactions on Medical Imaging, vol. 39, no. 6, pp. 2035\u20132050, Jun. 2020.\u00a0\u21a9</p> </li> </ol> <p>Parameters:</p> <ul> <li> <code>args</code>               (<code>Namespace</code>)           \u2013            <p>Arguments to configure the trainer.</p> </li> <li> <code>device</code>               (<code>device</code>)           \u2013            <p>Torch device to use for training.</p> </li> </ul>"},{"location":"reference/ldctbench/methods/qae/argparser/","title":"ldctbench.methods.qae.argparser","text":""},{"location":"reference/ldctbench/methods/qae/network/","title":"ldctbench.methods.qae.network","text":""},{"location":"reference/ldctbench/methods/qae/network/#ldctbench.methods.qae.network.Model","title":"<code>Model(args)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Quadratic Autoencoder (QAE)</p> <p>In the paper, the authors descibe their initialization as follows: As far as the Q-AE is concerned, parameters \\(w_r\\) and \\(w_g\\) of each layer were randomly initialized with a truncated Gaussian function, \\(b_g\\) are set to 1 for all the layers. In this way, quadratic term \\((w_r x^T + b_r )(w_g x^T + b_g)\\) turns into linear term \\((w_r x^T + b_r )\\). The reason why we use such initialization is because quadratic terms should not be pre-determined, they should be learned in the training. br and c were set to 0 initially for all the layers. wb was set to 0 here, we will discuss the influence of wb on the network in the context of direct initialization and transfer learning later.</p> <p>In our experiments, the network diverges with these settings. In their source code the authors also commented out the lines where they initialize \\(W_g\\) as truncated normal, and initialize it with zeros instead. We here follow their official GitHub implementation and initialize \\(W_g\\) as zero.</p>"},{"location":"reference/ldctbench/methods/redcnn/Trainer/","title":"ldctbench.methods.redcnn.Trainer","text":""},{"location":"reference/ldctbench/methods/redcnn/Trainer/#ldctbench.methods.redcnn.Trainer.Trainer","title":"<code>Trainer(args, device)</code>","text":"<p>               Bases: <code>BaseTrainer</code></p> <p>Trainer class for RED-CNN<sup>1</sup></p> <ol> <li> <p>H. Chen et al., \u201cLow-dose CT with a residual encoder-decoder convolutional neural network,\u201d IEEE Transactions on Medical Imaging, vol. 36, no. 12, pp. 2524\u20132535, Dec. 2017.\u00a0\u21a9</p> </li> </ol> <p>Parameters:</p> <ul> <li> <code>args</code>               (<code>Namespace</code>)           \u2013            <p>Arguments to configure the trainer.</p> </li> <li> <code>device</code>               (<code>device</code>)           \u2013            <p>Torch device to use for training.</p> </li> </ul>"},{"location":"reference/ldctbench/methods/redcnn/argparser/","title":"ldctbench.methods.redcnn.argparser","text":""},{"location":"reference/ldctbench/methods/redcnn/network/","title":"ldctbench.methods.redcnn.network","text":""},{"location":"reference/ldctbench/methods/redcnn/network/#ldctbench.methods.redcnn.network.Model","title":"<code>Model(args, out_ch=96)</code>","text":"<p>               Bases: <code>Module</code></p> <p>RED-CNN Model. Identical to the one proposed by the authors except for the final ReLU activation (to allow for zero mean-normalized data)</p>"},{"location":"reference/ldctbench/methods/resnet/Trainer/","title":"ldctbench.methods.resnet.Trainer","text":""},{"location":"reference/ldctbench/methods/resnet/Trainer/#ldctbench.methods.resnet.Trainer.Trainer","title":"<code>Trainer(args, device)</code>","text":"<p>               Bases: <code>BaseTrainer</code></p> <p>Trainer class for LDCT ResNet<sup>1</sup></p> <ol> <li> <p>A. D. Missert, S. Leng, L. Yu, and C. H. McCollough, \u201cNoise subtraction for low-dose CT images using a deep convolutional neural network,\u201d in Proceedings of the Fifth International Conference on Image Formation in X-Ray Computed Tomography, Salt Lake City, UT, USA, May 2018, pp. 399\u2013402.\u00a0\u21a9</p> </li> </ol> <p>Parameters:</p> <ul> <li> <code>args</code>               (<code>Namespace</code>)           \u2013            <p>Arguments to configure the trainer.</p> </li> <li> <code>device</code>               (<code>device</code>)           \u2013            <p>Torch device to use for training.</p> </li> </ul>"},{"location":"reference/ldctbench/methods/resnet/argparser/","title":"ldctbench.methods.resnet.argparser","text":""},{"location":"reference/ldctbench/methods/resnet/network/","title":"ldctbench.methods.resnet.network","text":""},{"location":"reference/ldctbench/methods/resnet/network/#ldctbench.methods.resnet.network.Model","title":"<code>Model(args, n_channels=128, n_blocks=10)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Residual network</p> <p>As proposed in the paper, the network performs noise subtraction on the input.</p> <p>Parameters:</p> <ul> <li> <code>args</code>               (<code>Namespace</code>)           \u2013            <p>Command line arguments passed to the model.</p> </li> <li> <code>n_channels</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Number of features for each conv layer, by default 128</p> </li> <li> <code>n_blocks</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of residual blocks, by default 10</p> </li> </ul>"},{"location":"reference/ldctbench/methods/resnet/network/#ldctbench.methods.resnet.network.ResBlock","title":"<code>ResBlock(ch)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Single Residual block</p> <p>Each block consists of Conv-&gt;BN-&gt;ReLU-&gt;GroupConv-&gt;BN-&gt;ReLU-&gt;Conv +                         |__________|</p> <p>Parameters:</p> <ul> <li> <code>ch</code>               (<code>int</code>)           \u2013            <p>Number of features to use for each conv layer.</p> </li> </ul>"},{"location":"reference/ldctbench/methods/transct/Trainer/","title":"ldctbench.methods.transct.Trainer","text":""},{"location":"reference/ldctbench/methods/transct/Trainer/#ldctbench.methods.transct.Trainer.Trainer","title":"<code>Trainer(args, device)</code>","text":"<p>               Bases: <code>BaseTrainer</code></p> <p>Trainer for TransCT<sup>1</sup>.</p> <ol> <li> <p>Z. Zhang, L. Yu, X. Liang, W. Zhao, and L. Xing, \u201cTransCT: Dual-path transformer for low dose computed tomography,\u201d in MICCAI, 2021.\u00a0\u21a9</p> </li> </ol> <p>Parameters:</p> <ul> <li> <code>args</code>               (<code>Namespace</code>)           \u2013            <p>Arguments to configure the trainer.</p> </li> <li> <code>device</code>               (<code>device</code>)           \u2013            <p>Torch device to use for training.</p> </li> </ul>"},{"location":"reference/ldctbench/methods/transct/argparser/","title":"ldctbench.methods.transct.argparser","text":""},{"location":"reference/ldctbench/methods/transct/network/","title":"ldctbench.methods.transct.network","text":""},{"location":"reference/ldctbench/methods/transct/network/#ldctbench.methods.transct.network.DepthToSpace","title":"<code>DepthToSpace(block_size)</code>","text":"<p>               Bases: <code>Module</code></p> <p>PyTorch implementation of TensorFlow's <code>tf.nn.depth_to_space</code> taken from here.</p>"},{"location":"reference/ldctbench/methods/transct/network/#ldctbench.methods.transct.network.Model","title":"<code>Model(args)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Implements the TransCT model.</p> <p>Translated from the Tensorflow implementation provided by the authors here.</p> <p>Parameters:</p> <ul> <li> <code>args</code>               (<code>Namespace</code>)           \u2013            <p>Command line arguments passed to the model.</p> </li> </ul>"},{"location":"reference/ldctbench/methods/transct/network/#ldctbench.methods.transct.network.SpaceToDepth","title":"<code>SpaceToDepth(block_size)</code>","text":"<p>               Bases: <code>Module</code></p> <p>PyTorch implementation of TensorFlow's <code>tf.nn.space_to_depth</code> taken from here.</p>"},{"location":"reference/ldctbench/methods/wganvgg/Trainer/","title":"ldctbench.methods.wganvgg.Trainer","text":""},{"location":"reference/ldctbench/methods/wganvgg/Trainer/#ldctbench.methods.wganvgg.Trainer.Trainer","title":"<code>Trainer(args, device)</code>","text":"<p>               Bases: <code>BaseTrainer</code></p> <p>Trainer for WGAN-VGG<sup>1</sup></p> <ol> <li> <p>Q. Yang et al., \u201cLow-dose CT image denoising using a generative adversarial network with wasserstein distance and perceptual loss,\u201d IEEE Transactions on Medical Imaging, vol. 37, no. 6, pp. 1348\u20131357, Jun. 2018.\u00a0\u21a9</p> </li> </ol> <p>Parameters:</p> <ul> <li> <code>args</code>               (<code>Namespace</code>)           \u2013            <p>Arguments to configure the trainer.</p> </li> <li> <code>device</code>               (<code>device</code>)           \u2013            <p>Torch device to use for training.</p> </li> </ul>"},{"location":"reference/ldctbench/methods/wganvgg/Trainer/#ldctbench.methods.wganvgg.Trainer.Trainer.gradient_penalty","title":"<code>gradient_penalty(target, fake, lam=10.0)</code>","text":"<p>Compute gradient penalty for given target and fake.</p> <p>Parameters:</p> <ul> <li> <code>target</code>               (<code>Tensor</code>)           \u2013            <p>Ground truth tensor</p> </li> <li> <code>fake</code>               (<code>Tensor</code>)           \u2013            <p>Fake = G(x) tensor</p> </li> <li> <code>lam</code>               (<code>float</code>, default:                   <code>10.0</code> )           \u2013            <p>lambda to weigh gradient penalty, by default 10.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Computed penalty using provided target, fake and self.critic</p> </li> </ul>"},{"location":"reference/ldctbench/methods/wganvgg/Trainer/#ldctbench.methods.wganvgg.Trainer.Trainer.train_step","title":"<code>train_step(batch)</code>","text":"<p>Training step</p> <p>Parameters:</p> <ul> <li> <code>batch</code>               (<code>Dict[str, Tensor]</code>)           \u2013            <p>Batch coming from training dataloader containing LD input and HD ground truth.</p> </li> </ul>"},{"location":"reference/ldctbench/methods/wganvgg/Trainer/#ldctbench.methods.wganvgg.Trainer.Trainer.val_step","title":"<code>val_step(batch_idx, batch)</code>","text":"<p>Validation step</p> <p>Parameters:</p> <ul> <li> <code>batch_idx</code>               (<code>int</code>)           \u2013            <p>Batch idx necessary for logging of samples.</p> </li> <li> <code>batch</code>               (<code>Dict[str, Tensor]</code>)           \u2013            <p>Batch coming from validation dataloader containing LD input and HD ground truth.</p> </li> </ul>"},{"location":"reference/ldctbench/methods/wganvgg/argparser/","title":"ldctbench.methods.wganvgg.argparser","text":""},{"location":"reference/ldctbench/methods/wganvgg/network/","title":"ldctbench.methods.wganvgg.network","text":""},{"location":"reference/ldctbench/methods/wganvgg/network/#ldctbench.methods.wganvgg.network.Discriminator","title":"<code>Discriminator(input_size)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Discriminator (Critic) for WGAN-VGG training</p> <p>Parameters:</p> <ul> <li> <code>input_size</code>               (<code>int</code>)           \u2013            <p>Input size of images fed in forward pass.</p> </li> </ul>"},{"location":"reference/ldctbench/methods/wganvgg/network/#ldctbench.methods.wganvgg.network.Discriminator.add_block","title":"<code>add_block(layers, ch_in, ch_out, stride)</code>  <code>staticmethod</code>","text":"<p>Append Conv -&gt; LeakyReLU block to layer list.</p> <p>Parameters:</p> <ul> <li> <code>layers</code>               (<code>List[Module]</code>)           \u2013            <p>List of layers</p> </li> <li> <code>ch_in</code>               (<code>int</code>)           \u2013            <p>Number of input features</p> </li> <li> <code>ch_out</code>               (<code>int</code>)           \u2013            <p>Number of output features</p> </li> <li> <code>stride</code>               (<code>int</code>)           \u2013            <p>Desired stride of the conv layer</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Module]</code>           \u2013            <p>Layer list with appended layer.</p> </li> </ul>"},{"location":"reference/ldctbench/methods/wganvgg/network/#ldctbench.methods.wganvgg.network.Discriminator.conv_output_size","title":"<code>conv_output_size(input_size, kernel_size_list, stride_list)</code>  <code>staticmethod</code>","text":"<p>Compute output size after feature extractor.</p> <p>Parameters:</p> <ul> <li> <code>input_size</code>               (<code>int</code>)           \u2013            <p>Input size of images fed in forward pass.</p> </li> <li> <code>kernel_size_list</code>               (<code>List[int]</code>)           \u2013            <p>List of kernel sizes for each layer.</p> </li> <li> <code>stride_list</code>               (<code>List[int]</code>)           \u2013            <p>List of strides for each layer.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>Output size after feature extractor.</p> </li> </ul>"},{"location":"reference/ldctbench/methods/wganvgg/network/#ldctbench.methods.wganvgg.network.Model","title":"<code>Model(args)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Generator for WGAN-VGG</p> <p>Parameters:</p> <ul> <li> <code>args</code>               (<code>Namespace</code>)           \u2013            <p>Command line arguments passed to the model.</p> </li> </ul>"},{"location":"reference/ldctbench/scripts/hpopt/","title":"ldctbench.scripts.hpopt","text":""},{"location":"reference/ldctbench/utils/argparser/","title":"ldctbench.utils.argparser","text":""},{"location":"reference/ldctbench/utils/auxiliaries/","title":"ldctbench.utils.auxiliaries","text":""},{"location":"reference/ldctbench/utils/auxiliaries/#ldctbench.utils.auxiliaries.apply_center_width","title":"<code>apply_center_width(x, center, width, out_range=(0, 1))</code>","text":"<p>Apply some center and width to an image</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>ndarray</code>)           \u2013            <p>Image array of arbitrary dimension</p> </li> <li> <code>center</code>               (<code>float</code>)           \u2013            <p>Float indicating the center</p> </li> <li> <code>width</code>               (<code>float</code>)           \u2013            <p>Float indicating the width</p> </li> <li> <code>out_range</code>               (<code>Tuple</code>, default:                   <code>(0, 1)</code> )           \u2013            <p>Desired output range, by default (0, 1)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>Copy of input array with center and width applied</p> </li> </ul>"},{"location":"reference/ldctbench/utils/auxiliaries/#ldctbench.utils.auxiliaries.dump_config","title":"<code>dump_config(args, path)</code>","text":"<p>Save argparse.Namespace to yaml file</p>"},{"location":"reference/ldctbench/utils/auxiliaries/#ldctbench.utils.auxiliaries.load_json","title":"<code>load_json(path)</code>","text":"<p>Load python object from json file</p>"},{"location":"reference/ldctbench/utils/auxiliaries/#ldctbench.utils.auxiliaries.load_obj","title":"<code>load_obj(path)</code>","text":"<p>Load python object from pickle file</p>"},{"location":"reference/ldctbench/utils/auxiliaries/#ldctbench.utils.auxiliaries.load_yaml","title":"<code>load_yaml(path)</code>","text":"<p>Load python object from yaml file</p>"},{"location":"reference/ldctbench/utils/auxiliaries/#ldctbench.utils.auxiliaries.save_json","title":"<code>save_json(content, path)</code>","text":"<p>Save python object to json file</p>"},{"location":"reference/ldctbench/utils/auxiliaries/#ldctbench.utils.auxiliaries.save_obj","title":"<code>save_obj(obj, path)</code>","text":"<p>Save python object to pickle file</p>"},{"location":"reference/ldctbench/utils/auxiliaries/#ldctbench.utils.auxiliaries.save_yaml","title":"<code>save_yaml(content, path)</code>","text":"<p>Save python object to yaml file</p>"},{"location":"reference/ldctbench/utils/metrics/","title":"ldctbench.utils.metrics","text":""},{"location":"reference/ldctbench/utils/metrics/#ldctbench.utils.metrics.Losses","title":"<code>Losses(dataloader, losses='loss')</code>","text":"<p>               Bases: <code>object</code></p> <p>Object to log losses</p> <p>Examples:</p> <p>In a training, losses can be logged as follows:</p> <pre><code>&gt;&gt;&gt; from ldctbench.utils.metrics import Losses\n&gt;&gt;&gt; # Setup training\n&gt;&gt;&gt; dataloader = {\"train\": DataLoader(Dataset(\"train\"), ...), \"val\": DataLoader(Dataset(\"val\"), ...)}\n&gt;&gt;&gt; losses = Losses(dataloader)\n&gt;&gt;&gt; # Perform training and validation routine\n&gt;&gt;&gt; for epoch in n_epochs:\n&gt;&gt;&gt;     # Train model\n&gt;&gt;&gt;     for batch in dataloader[\"train\"]:\n&gt;&gt;&gt;         x, y = batch\n&gt;&gt;&gt;         y_hat = model(x)\n&gt;&gt;&gt;         loss = criterion(y_hat, y)\n&gt;&gt;&gt;         # Perform weight optim\n&gt;&gt;          ...\n&gt;&gt;&gt;         losses.push(loss, \"train\")\n&gt;&gt;&gt;     losses.summarize(\"train\")\n&gt;&gt;&gt;     # Validate model\n&gt;&gt;&gt;     for batch in dataloader[\"val\"]:\n&gt;&gt;&gt;         x, y = batch\n&gt;&gt;&gt;         y_hat = model(x)\n&gt;&gt;&gt;         loss = criterion(y_hat, y)\n&gt;&gt;&gt;         losses.push(loss, \"val\")\n&gt;&gt;&gt;     losses.summarize(\"val\")\n&gt;&gt;&gt; # Log\n&gt;&gt;&gt; losses.log(savedir, epoch, 0)\n&gt;&gt;&gt; losses.plot(savedir)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>dataloader</code>               (<code>Dict[str, DataLoader]</code>)           \u2013            <p>Dict containing the dataloaders</p> </li> <li> <code>losses</code>               (<code>str</code>, default:                   <code>'loss'</code> )           \u2013            <p>Name of losses, by default \"loss\"</p> </li> </ul>"},{"location":"reference/ldctbench/utils/metrics/#ldctbench.utils.metrics.Losses.log","title":"<code>log(savepath, iteration, iterations_before_val)</code>","text":"<p>Log losses to wandb and local file</p> <p>Parameters:</p> <ul> <li> <code>savepath</code>               (<code>str</code>)           \u2013            <p>Where to store losses .csv file</p> </li> <li> <code>iteration</code>               (<code>int</code>)           \u2013            <p>Current iteration</p> </li> <li> <code>iterations_before_val</code>               (<code>int</code>)           \u2013            <p>Number of of training iterations before validation</p> </li> </ul>"},{"location":"reference/ldctbench/utils/metrics/#ldctbench.utils.metrics.Losses.plot","title":"<code>plot(savepath, y_log=True)</code>","text":"<p>Plot losses to a file</p> <p>Parameters:</p> <ul> <li> <code>savepath</code>               (<code>str</code>)           \u2013            <p>WHere to store pdf file</p> </li> <li> <code>y_log</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Plot y axis in logarithmic scale, by default True</p> </li> </ul>"},{"location":"reference/ldctbench/utils/metrics/#ldctbench.utils.metrics.Losses.push","title":"<code>push(loss, phase, name='loss')</code>","text":"<p>Push loss to object</p> <p>Parameters:</p> <ul> <li> <code>loss</code>               (<code>Union[Dict[str, Tensor], Tensor]</code>)           \u2013            <p>Single loss or dict of losses</p> </li> <li> <code>phase</code>               (<code>str</code>)           \u2013            <p>To which phase the loss(es) to push belongs</p> </li> <li> <code>name</code>               (<code>str</code>, default:                   <code>'loss'</code> )           \u2013            <p>Name of loss (only necessary if loss is not a dict), by default \"loss\"</p> </li> </ul>"},{"location":"reference/ldctbench/utils/metrics/#ldctbench.utils.metrics.Losses.reset","title":"<code>reset()</code>","text":"<p>Reset losses (add new epoch)</p>"},{"location":"reference/ldctbench/utils/metrics/#ldctbench.utils.metrics.Losses.summarize","title":"<code>summarize(phase)</code>","text":"<p>Summarize losses for this epoch</p> <p>Parameters:</p> <ul> <li> <code>phase</code>               (<code>str</code>)           \u2013            <p>For which phase to summarize loss</p> </li> </ul>"},{"location":"reference/ldctbench/utils/metrics/#ldctbench.utils.metrics.Metrics","title":"<code>Metrics(dataloader, metrics, denormalize_fn=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Object to log metrics</p> <p>Examples:</p> <p>In a training, metrics can be logged as follows:</p> <pre><code>&gt;&gt;&gt; from ldctbench.utils.metrics import Metrics\n&gt;&gt;&gt; # Setup training\n&gt;&gt;&gt; dataloader = {\"train\": DataLoader(Dataset(\"train\"), ...), \"val\": DataLoader(Dataset(\"val\"), ...)}\n&gt;&gt;&gt; metrics = Metrics(dataloader, metrics=[\"SSIM\", \"PSNR\", \"RMSE\"])\n&gt;&gt;&gt; # Perform training and validation routine\n&gt;&gt;&gt; for epoch in n_epochs:\n&gt;&gt;&gt;     # Train model\n&gt;&gt;&gt;     ...\n&gt;&gt;&gt;     # Validate model\n&gt;&gt;&gt;     for batch in dataloader[\"val\"]:\n&gt;&gt;&gt;         x, y = batch\n&gt;&gt;&gt;         y_hat = model(x)\n&gt;&gt;&gt;         metrics.push(y_hat, y)\n&gt;&gt;&gt;     metrics.summarize()\n&gt;&gt;&gt; # Log\n&gt;&gt;&gt; metrics.log(savedir, epoch,0)\n&gt;&gt;&gt; metrics.plot(savedir)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>dataloader</code>               (<code>Dict[str, DataLoader]</code>)           \u2013            <p>Dict containing the dataloaders</p> </li> <li> <code>metrics</code>               (<code>str</code>)           \u2013            <p>Name of metrics to log. Must be RMSE | SSIM | PSNR</p> </li> <li> <code>denormalize_fn</code>               (<code>Optional[Callable]</code>, default:                   <code>None</code> )           \u2013            <p>Function to use for denormalizing images before computing metrics, by default None</p> </li> </ul>"},{"location":"reference/ldctbench/utils/metrics/#ldctbench.utils.metrics.Metrics.log","title":"<code>log(savepath, iteration, iterations_before_val)</code>","text":"<p>Log metrics to wandb and local file</p> <p>Parameters:</p> <ul> <li> <code>savepath</code>               (<code>str</code>)           \u2013            <p>Where to store losses .csv file</p> </li> <li> <code>iteration</code>               (<code>int</code>)           \u2013            <p>Current iteration</p> </li> <li> <code>iterations_before_val</code>               (<code>int</code>)           \u2013            <p>Number of of training iterations before validation</p> </li> </ul>"},{"location":"reference/ldctbench/utils/metrics/#ldctbench.utils.metrics.Metrics.plot","title":"<code>plot(savepath)</code>","text":"<p>Plot metrics to a file</p> <p>Parameters:</p> <ul> <li> <code>savepath</code>               (<code>str</code>)           \u2013            <p>WHere to store pdf file</p> </li> </ul>"},{"location":"reference/ldctbench/utils/metrics/#ldctbench.utils.metrics.Metrics.push","title":"<code>push(targets, predictions)</code>","text":"<p>Compute metrics for given targets and predictions</p> <p>Parameters:</p> <ul> <li> <code>targets</code>               (<code>Union[ndarray, Tensor]</code>)           \u2013            <p>Ground truth reference</p> </li> <li> <code>predictions</code>               (<code>Union[ndarray, Tensor]</code>)           \u2013            <p>Prediction by the network</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If shape of predictions and targets are not identical</p> </li> <li> <code>ValueError</code>             \u2013            <p>If metric provided in init function is not in SSIM | PSNR | RMSE</p> </li> </ul>"},{"location":"reference/ldctbench/utils/metrics/#ldctbench.utils.metrics.Metrics.reset","title":"<code>reset()</code>","text":"<p>Reset metrics (start new epoch)</p>"},{"location":"reference/ldctbench/utils/metrics/#ldctbench.utils.metrics.Metrics.summarize","title":"<code>summarize()</code>","text":"<p>Summarize metric for this epoch</p>"},{"location":"reference/ldctbench/utils/training_utils/","title":"ldctbench.utils.training_utils","text":""},{"location":"reference/ldctbench/utils/training_utils/#ldctbench.utils.training_utils.PerceptualLoss","title":"<code>PerceptualLoss(network, device, in_ch=3, layers=[3, 8, 15, 22], norm='l1', return_features=False)</code>","text":"<p>               Bases: <code>Module</code></p> <p>The <code>layers</code> argument defines where to extract the activations. In the default paper, style losses are computed at: <code>3: \"relu1_2\"</code>, <code>8: \"relu2_2\"</code>, <code>15: \"relu3_3\"</code>, <code>22: \"relu4_3\"</code> and perceptual (content) loss is evaluated at: <code>15: \"relu3_3\"</code>. In<sup>1</sup> the content is evaluated in vgg19 after the 16th (last) conv layer (layer <code>34</code>)</p> <ol> <li> <p>Q. Yang et al., \u201cLow-dose CT image denoising using a generative adversarial network with wasserstein distance and perceptual loss,\u201d IEEE Transactions on Medical Imaging, vol. 37, no. 6, pp. 1348\u20131357, Jun. 2018.\u00a0\u21a9</p> </li> </ol> <p>Parameters:</p> <ul> <li> <code>network</code>               (<code>str</code>)           \u2013            <p>Which VGG flavor to use, must be \"vgg16\" or \"vgg19\"</p> </li> <li> <code>device</code>               (<code>device</code>)           \u2013            <p>Torch device to use</p> </li> <li> <code>in_ch</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Number of input channels, by default 3</p> </li> <li> <code>layers</code>               (<code>List[int]</code>, default:                   <code>[3, 8, 15, 22]</code> )           \u2013            <p>Number of layers at which to extract features, by default [3, 8, 15, 22]</p> </li> <li> <code>norm</code>               (<code>str</code>, default:                   <code>'l1'</code> )           \u2013            <p>Pixelwise, must be \"l1\" or \"mse\", by default \"l1\"</p> </li> <li> <code>return_features</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>description, by default False</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p><code>norm</code> is neither \"l1\" nor \"mse\".</p> </li> </ul>"},{"location":"reference/ldctbench/utils/training_utils/#ldctbench.utils.training_utils.repeat_ch","title":"<code>repeat_ch(in_ch)</code>","text":"<p>               Bases: <code>object</code></p> <p>Class to repeat input 3 times in channel dimension if <code>in_ch == 1</code></p> <p>Parameters:</p> <ul> <li> <code>in_ch</code>               (<code>int</code>)           \u2013            <p>Number of input channels.</p> </li> </ul>"},{"location":"reference/ldctbench/utils/training_utils/#ldctbench.utils.training_utils.setup_dataloader","title":"<code>setup_dataloader(args, datasets)</code>","text":"<p>Returns dict of dataloaders</p> <p>Parameters:</p> <ul> <li> <code>args</code>               (<code>Namespace</code>)           \u2013            <p>Command line arguments</p> </li> <li> <code>datasets</code>               (<code>Dict[str, Dataset]</code>)           \u2013            <p>Dictionary of datasets for each phase.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict[str, DataLoader]</code>           \u2013            <p>Dictionray of dataloaders for each phase.</p> </li> </ul>"},{"location":"reference/ldctbench/utils/training_utils/#ldctbench.utils.training_utils.setup_optimizer","title":"<code>setup_optimizer(args, parameters)</code>","text":"<p>Setup optimizer for given model parameters</p> <p>Parameters:</p> <ul> <li> <code>args</code>               (<code>Namespace</code>)           \u2013            <p>Command line arguments</p> </li> <li> <code>parameters</code>               (<code>Iterator[Parameter]</code>)           \u2013            <p>Parameters to be optimized. For some <code>model: nn.Module</code> these can be received via <code>model.parameters()</code></p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optimizer</code>           \u2013            <p>Optimizer for the given parameters</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If args.optimizer is not in \"sgd\" | \"adam\" | \"rmsprop\"</p> </li> </ul>"}]}